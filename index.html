---
title: 'Course Project: Machine Learning'
author: "Gustavo SÃ¡nchez"
date: "4/7/2020"
output:
    html_document: default
  
Introduction:
We will be using machine learning to predict the outcome of given data, when the training data is given.

Description:

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The main goal of the project is to predict the manner in which 6 participants performed the exercise.

Set Folder

```{r}
getwd() 
```

Reading the File:

```{r}
Dtrain<-read.csv("pml-training.csv")
Dtest<-read.csv("pml-testing.csv")
```
Importing libraries:
```{r}
library(rpart)
library(randomForest)
library(glm2)
library(caret)
library(ggplot2)
library(lattice)
```

Data cleaning:
Data contains a lot of NA, which we will remove. Variables that have near zero variance are also removed to make prediction more accurate.
```{r}
# Removing near zero variance
NZV <- nearZeroVar(Dtrain)
Dtrain <- Dtrain[, -NZV]
Dtest  <- Dtest[, -NZV]
# Removing variables that contain mostly NA
AllNA    <- sapply(Dtrain, function(x) mean(is.na(x))) > 0.95
Dtrain <- Dtrain[, AllNA==FALSE]
Dtest  <- Dtest[, AllNA==FALSE]
# Remove variables that are used as identification only,
# these should not impact the prediction in any way, but it will,
# if we don't remove it.
Dtrain <- Dtrain[, -(1:5)]
Dtest  <- Dtest[, -(1:5)]
dim(Dtrain)
```
Exploratory Analysis:

Let us start with splitting data for cross validation.
```{r}
set.seed(1230) #To make the results reproducible
intrain<-createDataPartition(Dtrain$classe,p=0.7,list=FALSE)
subtrain<-Dtrain[intrain,]
subtest<-Dtrain[-intrain,]
```

Prediction:
We will use below 3 methods to predict the result for test set.

1.Boosting

2.Decision Trees

3.Random Forest

We will use all variables as predictors.

Accuracy of each prediction model is determined by cross-validation method. This works by splitting training data into two sub groups where one is for training and other for testing. We have used 70:30 ratio for splitting training data.

1. Boosted  prediction model:

```{r}
library(gbm)
library(glm2)
set.seed(1230)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
##Careful: Below line of code takes a long time to run
gbm_model<-train(classe~.,data=subtrain,method="gbm",trControl = control,verbose=FALSE)
pred_values3<-predict(gbm_model,newdata=subtest)
confusionMatrix(as.factor(subtest$classe),pred_values3)
```
This model has given us 98.49% accuracy

2. Decision Trees for prediction:

```{r}
library(rpart) # ignore this line
set.seed(1230)
linear_model<-rpart(classe~.,data=subtrain,method="class")
pred_values2<-predict(linear_model,newdata=subtest,type="class")
confusionMatrix(as.factor(subtest$classe),pred_values2)
```
This method gives on 74.55% accuracy

3. Random Forest Method

```{r}
set.seed(1230)
control <- trainControl(method="cv", number=3, verboseIter=FALSE)
##Careful: Below line of code takes a long time to run
forest_model<-train(classe~.,data=subtrain,method="rf",trControl=control)
```

```{r}
pred_values1<-predict(forest_model,newdata=subtest)
confusionMatrix(as.factor(subtest$classe),pred_values1)
```
We have achieved an accuracy of 99.75%

Prediction Results:
Accuracy of each method:

Random Forests: 99.75% accuracy

Boosted model : 99.49% accuracy

Decision Trees: 74.55% accuracy



The best model for prediction is random forest

So we will use that method for predicting test set



Results of test data:
```{r}
predict(forest_model,Dtest)
```

